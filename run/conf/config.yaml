# ---------- Overriding hydra default configs ----------
hydra:
  job:
    name: rsna_mammo
  run:
    dir: ${out_dir}

# ---------- Default settings ----------
defaults:
  - dataset: rsna_mammo
  - optimizer: adam
  - scheduler: cosine

  # For hydra colorlog
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


# ---------- Other configs ----------

#====
# Preprocessing
#====
preprocessing:
  h_resize_to: 1024
  w_resize_to: 512
  mean: 0.485 # [0.485, 0.456, 0.406]
  std: 0.229 # [0.229, 0.224, 0.225]


#====
# Model
#====
model:
  restore_path: ${test_model}  # Restore model's weight from this path

  base_model: convnext_small.fb_in22k_ft_in1k  # Pretrained model to load
  backbone_class: null
  freeze_backbone: false

  head: ${head}
  output_dim: ${dataset.num_classes}
  pool: ${pool}
  use_bn: false
  in_chans: 2
  grad_checkpointing: false


pool:
  type: gem_ch # adaptive or gem or gem_ch
  # gem
  p: 3
  p_trainable: true

#====
# Model head
#====
head:
  type: linear


#====
# Forwarder
#====
forwarder:
  loss:
    cancer_weight: 1.0
    biopsy_weight: 0.0
    invasive_weight: 0.0
    age_weight: 0.0
    machine_id_weight: 0.0
    site_id_weight: 0.0

#====
# Dataset
#====
dataset:
  type: ???
  num_classes: ???


#====
# Data augmentation
# Should have effect only in training (not in validation nor test)
#====
# augmentation: null
augmentation:
  use_aug: false
  use_heavy_aug: false
  rotate: 15
  translate: 0.25
  shear: 3
  p_affine: 0.5
  crop_scale: 0.9
  crop_l: 0.75
  crop_r: 1.3333333333333333
  p_gray: 0.1
  p_blur: 0.05
  p_noise: 0.05
  p_downscale: 0.0
  p_shuffle: 0.3
  p_posterize: 0.2
  p_bright_contrast: 0.5
  p_cutout: 0.05

#====
# Training
#====
training:
  project_name: rsna_mammo
  resume_from: null  # If set, restore all training state from that checkpoint
  debug: false  # If true, run in a debug mode
  use_wandb: false # If true, WandbLogger will be used
  seed: 0  # Random seed passed for seed_everything
  monitor: val/auc
  monitor_mode: max
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1
  save_embed: false

  epoch: 10
  batch_size: 16
  batch_size_test: 16
  num_gpus: 1
  num_workers: 12
  drop_last: true  # If true, drop the last incomplete batch in training phase
  use_amp: true  # If true, use 16-bit precision for training


#====
# Optimizer
#====
optimizer:
  type: ???
  lr_head: ${optimizer.lr}


#====
# Scheduler
#====
scheduler:
  type: ???
  num_steps_per_epoch: null


#====
# Other essential configs
#====
out_dir: ???
test_model: null  # If set, only run test with that model
